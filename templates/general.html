<!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
        <title>{{ title }}</title>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.5.0/Chart.js"></script>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
        <!-- Include all compiled plugins (below), or include individual files as needed -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <!-- Bootstrap -->
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
        <style>
            body { padding-top: 50px; }
        </style>
        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->
      </head>
         <body>

            <nav class="navbar navbar-inverse navbar-fixed-top">
              <div class="container">
                <div class="navbar-header">
                  <a class="navbar-brand" href="index.html">Billion-scale ANN Benchmarks</a>
                </div>
                <div id="navbar" class="collapse navbar-collapse">
                  <ul class="nav navbar-nav">
                    <li class="active"><a href="index.html">Home</a></li>
                  </ul>
                  <ul class="nav navbar-nav">
                    <li class="active"><a href="index.html#why">Goals</a></li>
                  </ul>
                  <ul class="nav navbar-nav">
                    <li class="active"><a href="index.html#tracks">Tracks</a></li>
                  </ul>
                  <ul class="nav navbar-nav">
                    <li class="active"><a href="index.html#bench-datasets">Datasets</a></li>
                  </ul>
                  <ul class="nav navbar-nav">
                    <li class="active"><a href="index.html#call">CFP</a></li>
                  </ul>
                  <!-- <ul class="nav navbar-nav">
                    <li class="active"><a href="index.html#datasets">Results by Datasets</a></li>
                  </ul>
                  <ul class="nav navbar-nav">
                    <li class="active"><a href="index.html#algorithms">Results by Algorithms</a></li>
                  </ul> -->
                  <ul class="nav navbar-nav">
                    <li class="active"><a href="index.html#organizers">Organizers</a></li>
                  </ul>
                </div><!--/.nav-collapse -->
              </div>
            </nav>

            {% block content %} {% endblock %}

            <div id="why">
              <h2>Why this competition?</h2>
              In the past few years, we’ve seen a lot of new research and creative approaches for large-scale ANNS, including:
              <ul>
                <li>Partition-based, and graph-based indexing strategies (as well as hybrid indexing approaches).</li>
                <li>Mixing RAM and SSD storage to efficiently store and process large datasets that exceed the size of RAM.</li>
                <li>Using accelerator hardware such as GPUs, FPGAs, and other custom in-memory silicon.</li>
                <li>Leveraging machine learning for dimension reduction of the original vectors.</li>
              </ul>
              <p>
               In addition to an uptick in academic interest, many implementation of these algorithms at scale now appear in production 
               and high availability datacenter contexts: powering enterprise-grade, mission-critical, and web-scale search applications.
                In these deployment scenarios, benchmarks such as cost, preprocessing time, power consumption become just as important as
                the accuracy-vs-latency tradeoff. Despite this, most empirical evaluations of algorithms have focused on smaller datasets
                of about a million points, e.g. ann-bechmarks.com. However, deploying recent algorithmic advances in ANNS techniques for
                search, recommendation and ranking at scale require support at billion or substantially larger scale. Barring a few recent
                papers, there is limited consensus on which algorithms are effective at this scale.
              </p>

                We believe that this challenge will be impactful in several ways:
                <ul>
                  <li>Provides a comparative understanding of algorithmic ideas and their application at scale.</li>
                  <li>Promotes the development of new techniques for the problem and demonstration of their value.</li>
                  <li>Provides a compilation of datasets, many new, to enable future development of algorithms.</li>
                  <li>Introduces of a standard benchmarking approach.</li>
                </ul>
                By providing a platform for those interested in this problem, we aim to encourge more collaboration and collectively advance the field at a more rapid pace.
                <strong>Limited Azure compute credit will be provided to researchers in need of compute to develop their ideas.</strong>
            </div>

            <div id="tracks">
              <h2>Tracks</h2>
              <h4>Standard Track</h4>
              <p>There are two standard standard hardware tracks in which the trade-off between recall and throughput will be evaluated. 
              <ul>
                <li>(T1) In-memory indices with <a href="https://github.com/facebookresearch/faiss" target="_blank">FAISS</a> as the baseline. DRAM is constrained to 64GB for search and 128GB for build. </li>
                <li>(T2) Out-of-core indices  with <a href="https://github.com/Microsoft/diskann" target="_blank">DiskANN</a> as the baseline. In addition to the limited DRAM in T1, the machine will have also 1TB SSD storage for search and 3TB storage for build.</li>
              </ul>
              Participants are expected to release their code for index building and search which the organizers will run on separate machines.
              Participants provide a configuration for their index build code that would complete in 4 days on an Azure
              <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/fsv2-series">Standard_F64s_v2 VM</a>
              with 4TB of SSD to be used for storing the data, index and other intermediate data (details likely to change).
              For search, participants are allowed up to 10 hyperparameter configurations. 
              The protocol for evaluation is as follows:
              <ul type="none">
                <li>[on indexing machine] participants will be given a local path with 1B vector dataset. </li>
                <li>[on indexing machine] participants build an index from the 1B vectors and store back to local disk. </li>
                <li>[on indexing machine] Stored index is copied out to a temporary cloud storage location by the eval framework.</li>
                <li>[on search machine] organizers load the index from temp blob to a local path and provide the path to the search code.</li>
                <li>[on search machine] organizers perform searches with held-out query set and measure accuracy and search time with several sets of parameters.</li>
              </ul>
              </p>
              Finalized details for build and search hardware timing will be released along with the the eval framework.

              <h4> Custom Hardware Track</h4>
              <p>
              For the Custom Hardware track (T3), the participants can use non-standard hardware such as GPUs, AI accelerators, FPGAs, and custom in-memory silicon. 
              In this track, participants will either 1) send their hardware, such as PCI boards, to GSI technology at their expense, 
              or 2) if that is not possible, we will make scripts available so that those participants can run the evaluation themselves. 
              In the T3 track, we will collect the following benchmarks:
              <ul>
                <li>Search Recall vs throughput</li>
                <li>Index build time</li>
                <li>Cost and power (measured as queries/second/watt and MSRP/watt)</li>
              </ul>
               We will ask the participant to provide details and proof of the custom hardware’s cost in the form of manufactured suggested retail price (MSRP).
              </p>
               We will provide the exact details on how we collect and compute these benchmarks as well as additional machine and operating system specification before the competition begins.
            </div>

            <div id="bench-datasets">
              <h2>Benchmark Datasets</h2>
              We intend to use the following 6 billion point datasets (datasets are subject to change).  
              <ul>
                <li><a href="http://corpus-texmex.irisa.fr/" target="_blank">BIGANN</a> consists of SIFT descriptors applied to images from extracted from a large image dataset.</li>
                <li>Facebook-simsearchnet is a new dataset being released by Facebook for this competition.</li>
                <li>Microsoft-Turing-ANNS is a new dataset being released by the Microsoft Turing team for this competition.</li>
                <li><a href="https://github.com/microsoft/SPTAG/tree/master/datasets/SPACEV1B">Microsoft SPACEV1B</a> is a new web search related dataset being released by Microsoft Bing for this competition.
                 It consists of document and query vectors encoded by Microsoft SpaceV Superior model to capture generic intent representation.</li>
                <li><a href="https://research.yandex.com/datasets/biganns" target="_blank">Yandex DEEP1B</a> image descriptor dataset consisting of the projected
                  and normalized outputs from the last fully-connected layer of the GoogLeNet model, which was pretrained on the Imagenet classification task.  </li>
                <li><a href="https://research.yandex.com/datasets/biganns" target="_blank">Yandex Text-to-Image-1B</a> is a new cross-model dataset (text and visual),
                  where database and query vectors can potentially have different distributions in a shared representation space. Image embeddings produced by the
                  Se-ResNext-101 model, and queries are textual embeddings produced by a variant of the DSSM model. </li>
              </ul>

              <p>
              All datasets including ground truth data are in the standardized binary format that starts with 8 bytes of data consisting of <em>num_points(uint32_t)</em>
              <em>num_dimensions(uint32)</em> followed by <em>num_pts X num_dimensions x sizeof(type)</em> bytes of data stored one vector after another. Data files
              will have suffixes <em>.fbin</em>, <em>.u8bin</em>, and <em>.i8bin</em> to represent float32, uint8 and int8 type data. Note that a different query set
              will be used for evaluation. The details of the datasets along with links to the base, query and sample sets, and the ground truth nearest neighbors 
              of the query set are listed below. 
              </p>

              <TABLE  style="text-align:center"  border="1" width="800" cellpadding=5>
                <TR>
                   <TD> Dataset </TD>
                   <TD> Datatype </TD>
                   <TD> Dimensions </TD>
                   <TD> Distance </TD>
                   <TD> Base data </TD>
                   <TD> Sample data </TD>
                   <TD> Query data </TD>
                   <TD> Ground Truth </TD>
                   <TD> Release terms </TD>
                </TR>
                <TR>
                   <TD> BIGANN </TD>
                   <TD> uint8 </TD>
                   <TD> 128 </TD>
                   <TD> L2 </TD>
                   <TD> TBA </TD>
                   <TD> TBA </TD>
                   <TD> TBA </TD>
                   <TD> TBA </TD>
                   <TD> TBA </TD>
                </TR>
                <TR>
                  <TD> Facebook-simsearchnet* </TD>
                  <TD> uint8 </TD>
                  <TD> 256 </TD>
                  <TD> cosine </TD>
                  <TD> TBA </TD>
                  <TD> TBA </TD>
                  <TD> TBA </TD>
                  <TD> TBA </TD>
                  <TD> TBA </TD>
               </TR>
               <TR>
                <TD> Microsoft-Turing-ANNS* </TD>
                <TD> int8 </TD>
                <TD> 100 </TD>
                <TD> L2 </TD>
                <TD> TBA </TD>
                <TD> TBA </TD>
                <TD> TBA </TD>
                <TD> TBA </TD>
                <TD> TBA </TD>
              </TR>
              <TR>
                <TD> Microsoft-SPACEV1B* </TD>
                <TD> int8 </TD>
                <TD> 100 </TD>
                <TD> L2 </TD>
                <TD> TBA </TD>
                <TD> TBA </TD>
                <TD> TBA </TD>
                <TD> TBA </TD>
                <TD> TBA </TD>
              </TR>
              <TR>
                <TD> Yandex DEEP1B </TD>
                <TD> float32 </TD>
                <TD> 96 </TD>
                <TD> L2 </TD>
                <TD> <a href="https://storage.yandexcloud.net/yandex-research/ann-datasets/DEEP/base.1B.fbin">1B points</a> </TD>
                <TD> <a href="https://storage.yandexcloud.net/yandex-research/ann-datasets/DEEP/learn.350M.fbin ">350M points</a> </TD>
                <TD> <a href="https://storage.yandexcloud.net/yandex-research/ann-datasets/DEEP/query.public.10K.fbin">10K queries</a> </TD>
                <TD> <a href="https://storage.yandexcloud.net/yandex-research/ann-datasets/DEEP/groundtruth.public.10K.ibin">link</a> </TD>
                <TD> <a href="https://creativecommons.org/licenses/by/4.0/deed.en">CC BY 4.0</a> </TD>
              </TR>
              <TR>
                <TD> Yandex Text-to-Image-1B* </TD>
                <TD> float32 </TD>
                <TD> 200 </TD>
                <TD> inner-product </TD>
                <TD> <a href="https://storage.yandexcloud.net/yandex-research/ann-datasets/T2I/base.1B.fdata">1B points</a> </TD>
                <TD> <a href="https://storage.yandexcloud.net/yandex-research/ann-datasets/T2I/query.learn.50M.fbin ">50M points</a> </TD>
                <TD> <a href="https://storage.yandexcloud.net/yandex-research/ann-datasets/T2I/query.public.100K.fbin">100K queries</a> </TD>
                <TD> <a href="https://storage.yandexcloud.net/yandex-research/ann-datasets/T2I/groundtruth.public.100K.ibin">link</a> </TD>
                <TD> <a href="https://creativecommons.org/licenses/by/4.0/deed.en">CC BY 4.0</a> </TD>
              </TR>
             </TABLE>
             * new datasets
            </div>

            <div id="call">
              <h2>Call for Participation and Timeline</h2>
                Watch this space for official registration, finalized datasets, and competition rules. We expect registration in May. <BR>
                Rough Timeline (subject to change):
                <ul>
                  <li>May: release of data, guidelines, rules, and a call for participation. The competition registration web site goes on-line.</li>
                  <li>Mid-June: Baseline results and testing infrastructure released.</li>
                  <li>June 30th: Participants in need of compute resources to submit an expression of interest.</li>
                  <li>mid-July: Allocation of compute resources and start of the competition timeline.</li>
                  <li>July 30th: Final deadline for participants to submit an expression of interest through CMT.</li>
                  <li>October 22nd: End of competition period and submission of code.</li>
                  <li>October 29th: Participants submit a brief report outlining their algorithm and results.</li>
                  <li>Mid-November: Release of preliminary results on standardized machines. Review of code by organizers and participants. Participants can raise concerns about the evaluation.</li>
                  <li>Early December: Final results published, and competition results archived (the competition will go on if interest continues).</li>
                </ul>
                During NeurIPS Organizers overview the competition and results. Organizers also request the best entries
                (including leaderboard toppers, or promising new approaches) to present an overview followed by discussion.
            </div>

            <div id="organizers">
              <h2>Organizers and Dataset Contributors</h2>
              <ul>
                <li><a href="http://harsha-simhadri.org/" target="_blank">Harsha Vardhan Simhadri, Microsoft Research India</a></li>
                <li><a href="https://medium.com/@georgewilliams" target="_blank">George Williams, GSI Technology</a> </li>
                <li><a href="http://www.itu.dk/people/maau/" target="_blank">Martin Aumüller, IT University of Copenhagen</a> </li>
                <li><a href="https://research.yandex.com/people/102794/" target="_blank">Artem Babenko, Yandex</a></li>
                <li><a href="https://research.yandex.com/people/610754" target="_blank">Dmitry Baranchuk, Yandex</a></li>
                <li><a href="https://www.microsoft.com/en-us/research/people/cheqi/" target="_blank">Qi Chen, Microsoft Research Asia</a></li>
                <li><a href="https://ai.facebook.com/people/matthijs-douze/" target="_blank">Matthijs Douze, Facebook AI Research</a></li>
                <li><a href="https://github.com/beauby/" target="_blank">Lucas Hosseini, Facebook AI Research</a></li>
                <li><a href="https://rakri.github.io/" target="_blank">Ravishankar Krishnaswamy, Microsoft Research India and IIT Madras</a></li>
                <li><a href="https://www.microsoft.com/en-us/research/people/gopalsr//" target="_blank">Gopal Srinivasa, Microsoft Research India</a></li>
                <li><a href="https://suhasjs.github.io/" target="_blank">Suhas Jayaram Subramanya, Carnegie Mellon University</a></li>
                <li><a href="https://www.microsoft.com/en-us/research/people/jingdw/" target="_blank">Jingdong Wang, Microsoft Research Asia</a></li>
              </ul>

            <p>
              Organizers can be reached at <a href="mailto:big-ann-organizers@googlegroups.com">big-ann-organizers@googlegroups.com</a>.
            </p>
            <p>
              We are grateful to Microsoft Research for helping organize this competition, and contributing compute credits.
            </p>
            </div>
        </div>
    </body>
</html>
